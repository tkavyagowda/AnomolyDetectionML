# -*- coding: utf-8 -*-
"""new_dataset_ lstm_anomaly_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aGjQa6IxMQRUXm_m41Ut8_l_SkG8mMLp
"""

#imports_part_1.py
from pathlib import Path # convenient way to deal w/ paths
import plotly.graph_objects as go # creates plots
import numpy as np # standard for data processing
import pandas as pd # standard for data processing
import json # we have anomalies' timestamps in json format

from google.colab import drive
drive.mount('/content/drive')

labels_filepath = '/content/drive/MyDrive/labels.json'
 
training_filename = '/content/drive/MyDrive/train.csv'
 
valid_filename = '/content/drive/MyDrive/test.csv'

#labels_loading.py
with open(labels_filepath, 'r') as f:
    anomalies_timestamps = json.load(f)

#read_data.py 
train = pd.read_csv(training_filename)
valid = pd.read_csv(valid_filename)

train.head()

valid.head()

from sklearn.preprocessing import StandardScaler

def parse_and_standardize(df: pd.DataFrame, scaler: StandardScaler = None):
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['stand_value'] = df['value']
    if not scaler:
        scaler = StandardScaler()
        scaler.fit(df['stand_value'].values.reshape(-1, 1))
    df['stand_value'] = scaler.transform(df['stand_value'].values.reshape(-1, 1))
    return scaler

data_scaler = parse_and_standardize(train)
parse_and_standardize(valid, data_scaler)

"""## Get anomalies from the data"""

train_anomalies = train[train['timestamp'].isin(anomalies_timestamps[training_filename])]
valid_anomalies = valid[valid['timestamp'].isin(anomalies_timestamps[valid_filename])]

train_anomalies

valid_anomalies

"""## Plot data with anomalies

### Training data
"""

import plotly.graph_objects as go

layout = dict(xaxis=dict(title='Timestamp'), yaxis=dict(title='Battery Temperature'))
fig = go.Figure(layout=layout)
fig.add_trace(go.Scatter(x=train['timestamp'], y=train['value'], 
                         mode='markers', name='Non-anomaly',
                         marker=dict(color='blue')))
fig.add_trace(go.Scatter(x=train_anomalies['timestamp'], y=train_anomalies['value'], 
                         mode='markers', name='Anomaly',
                         marker=dict(color='green', size=13)))

"""### Validation data"""

fig = go.Figure()
fig.add_trace(go.Scatter(x=valid['timestamp'], y=valid['value'], 
                         mode='markers', name='Non-anomaly',
                         marker=dict(color='blue')))
fig.add_trace(go.Scatter(x=valid_anomalies['timestamp'], y=valid_anomalies['value'], 
                         mode='markers', name='Anomaly',
                         marker=dict(color='green', size=13)))

"""## Label anomalies and non-anomalies """

train['anomaly'] = 0
train.loc[train_anomalies.index, 'anomaly'] = 1
train.iloc[train_anomalies.index]

valid['anomaly'] = 0
valid.loc[valid_anomalies.index, 'anomaly'] = 1
valid.iloc[valid_anomalies.index]

train.head()

"""# Dataset Preparation for LSTM"""

import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

class CPUDataset(Dataset):
    def __init__(self, data: pd.DataFrame, size: int):
        self.chunks = torch.FloatTensor(data['stand_value']).unfold(0, size, size)
        
    def __len__(self):
        return self.chunks.size(0)
    
    def __getitem__(self, i):
        x = self.chunks[i]
        return x

train_values = train['stand_value'].values.astype(np.float32).flatten()
valid_values = valid['stand_value'].values.astype(np.float32).flatten()

train_ds = CPUDataset(train, 64)
valid_ds = CPUDataset(valid, 64)

train_dataloader = DataLoader(train_ds, batch_size=1)
validation_dataloader = DataLoader(valid_ds, batch_size=1)

"""# LSTM Architecture"""

class LSTMModel(nn.Module):
    def __init__(self, in_size, hidden_size, out_size, device):
        super().__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(in_size, hidden_size)
        self.linear = nn.Linear(hidden_size, out_size)
        self.device = device
        self.init_hidden()
        
    def forward(self, x):
        out, self.hidden_state = self.lstm(x.view(len(x), 1, -1), self.hidden_state)
        self.hidden_state = tuple([h.detach() for h in self.hidden_state])
        out = out.view(len(x), -1)
        out = self.linear(out)
        return out
    
    def init_hidden(self):
        self.hidden_state = (torch.zeros((1, 1, self.hidden_size)).to(self.device),
                             torch.zeros((1, 1, self.hidden_size)).to(self.device))

"""# Training of the model"""

from tqdm.notebook import tqdm
import torch.optim as opt
import math
import copy

"""Definition of the training loop"""

def train_model(model: LSTMModel, dataloaders: dict, optimizer: opt.Optimizer, 
                scheduler, criterion, device: torch.device, epochs: int):
    losses_data = {'train': [], 'valid': []}
    model.to(device)
    for epoch in tqdm(range(epochs)):
        print(f'Epoch {epoch}/{epochs-1}')
        for phase in ['train', 'valid']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.
            running_total = 0.
            
            for idx, sequence in enumerate(dataloaders[phase]):
                value = sequence
                value = value.to(device)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    out = model(value.view(-1, 1))
                    loss = criterion(out.view(-1), value.view(-1))

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                        scheduler.step()

                running_loss += loss.item() * out.size(0)
                running_total += out.size(0)

            epoch_loss = running_loss / running_total
            print(f'{phase.capitalize()} Loss: {epoch_loss}')
            losses_data[phase].append(epoch_loss)
    return losses_data

"""Initialization of the model, dataloaders and training parameters"""

total_epoch_count = 50

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LSTMModel(1, 128, 1, device)
model = model.to(device)

dataloaders = {
    'train': train_dataloader,
    'valid': validation_dataloader
}

optimizer = opt.Adam(params=model.parameters(), lr=1e-3)
sched = opt.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(dataloaders['train']), epochs=total_epoch_count)
criterion = nn.MSELoss()

"""Training"""

losses_dict = train_model(
    model,
    dataloaders,
    optimizer,
    sched,
    criterion,
    device,
    total_epoch_count
)

"""Plot of the training and validation losses"""

layout = dict(xaxis=dict(title='Epoch'), yaxis=dict(title='Loss'))
fig = go.Figure(layout=layout)
fig.add_trace(go.Scatter(y=losses_dict['train'], mode='lines', name='Train Loss',))
fig.add_trace(go.Scatter(y=losses_dict['valid'], mode='lines', name='Valid Loss'))

print(min(*losses_dict['train']))
print(min(*losses_dict['valid']))

"""# Results Evaluation

## Plot of the 'pure' result for train data

Getting of the 'pure' result
"""

model.eval()
with torch.no_grad():
    res_train = model(torch.tensor(train_values).to(device))
res_train = res_train.cpu()

"""Plot of the result:


*   Blue points - real values
*   Orange points - predicted values
"""

layout = dict(xaxis=dict(title='Timestamp'), yaxis=dict(title='Battery Temperature'))
fig = go.Figure(layout=layout)
fig.add_trace(go.Scatter(x=train['timestamp'], y=train['stand_value'], 
                         mode='markers', name='Ground Truth',
                         marker=dict(color='blue')))
fig.add_trace(go.Scatter(x=train['timestamp'], y=torch.flatten(res_train), 
                         mode='markers', name='Predicted Value', 
                         marker=dict(color='orange')))

"""## Plot of the 'pure' result for valid data

Getting of the 'pure' result
"""

model.eval()
with torch.no_grad():
    res_valid = model(torch.tensor(valid_values).to(device))
res_valid = res_valid.cpu()

"""Plot of the result:


*   Blue points - real values
*   Orange points - predicted values
"""

layout = dict(xaxis=dict(title='Timestamp'), yaxis=dict(title='Battery Temperature'))
fig = go.Figure(layout=layout)
fig.add_trace(go.Scatter(x=valid['timestamp'], y=valid['stand_value'], 
                         mode='markers', name='Ground Truth',
                         marker=dict(color='blue')))
fig.add_trace(go.Scatter(x=valid['timestamp'], y=torch.flatten(res_valid), 
                         mode='markers', name='Predicted Value', 
                         marker=dict(color='orange')))

"""## Anomaly detection with one threshold

We use **three-sigma rule** applied to model's prediction errors to detect anomalies

### Threshold calculation

Calculation of the prediction errors for **training** data *(and only for training)*
"""

def calculate_prediction_errors(target, predicted, criterion):
    reconstruction_errors = []
    for t, p in zip(target, predicted):
        reconstruction_errors = np.append(
            reconstruction_errors, criterion(p, t).cpu().numpy().flatten()
        )

    return reconstruction_errors

train_pred_errors = calculate_prediction_errors(res_train.view(-1), torch.tensor(train_values).view(-1), criterion)
train_pred_errors.shape

np.mean(train_pred_errors), np.std(train_pred_errors)

"""The threshold is calculated as the mean of the prediction errors + 3 standard deviations of them"""

pred_error_threshold = np.mean(train_pred_errors) + 3 * np.std(train_pred_errors)
pred_error_threshold

"""### Data filtering

filter results of the model according to the threshold and get the indexes of detected anomalies
"""

def detect_anomalies(predicted: torch.Tensor, target, threshold: float, criterion):
    anomalies_idxs = []
    for idx, (t, p) in enumerate(zip(target, predicted)):
        error = criterion(p.view(-1), t.view(-1)).cpu().numpy().flatten()
        if error > threshold:
            anomalies_idxs.append(idx)
    return anomalies_idxs

train_anomalies_idxs = detect_anomalies(res_train, torch.tensor(train_values), pred_error_threshold, criterion)
train_anomalies_idxs

"""Plot of the result for training data:


*   Blue points - non-anomaly data
*   Red points - detected anomaly data
*   Green points - real anomaly data
"""

layout = dict(xaxis=dict(title='Timestamp'), yaxis=dict(title='Battery Temperature'))
fig = go.Figure(layout=layout)
fig.add_trace(go.Scatter(x=train['timestamp'], y=train['value'], 
                         mode='markers', name='Non-anomaly',
                         marker=dict(color='blue')))
fig.add_trace(go.Scatter(x=train_anomalies['timestamp'], y=train_anomalies['value'], 
                         mode='markers', name='Real Anomaly',
                         marker=dict(color='green', size=13)))
fig.add_trace(go.Scatter(x=train['timestamp'][train_anomalies_idxs],
                         y=train['value'][train_anomalies_idxs], 
                         mode='markers', name='Detected Anomaly',
                         marker=dict(color='red', size=7)))

valid_anomalies_idxs = detect_anomalies(res_valid, torch.tensor(valid_values), pred_error_threshold, criterion)
valid_anomalies_idxs

"""Plot of the result for validation data:


*   Blue points - non-anomaly data
*   Red points - detected anomaly data
*   Green points - real anomaly data
"""

layout = dict(xaxis=dict(title='Timestamp'), yaxis=dict(title='Battery Temperature'))
fig = go.Figure(layout=layout)
fig.add_trace(go.Scatter(x=valid['timestamp'], y=valid['value'], 
                         mode='markers', name='Non-anomaly',
                         marker=dict(color='blue')))
fig.add_trace(go.Scatter(x=valid_anomalies['timestamp'], y=valid_anomalies['value'], 
                         mode='markers', name='Real Anomaly',
                         marker=dict(color='green', size=13)))
fig.add_trace(go.Scatter(x=valid['timestamp'][valid_anomalies_idxs],
                         y=valid['value'][valid_anomalies_idxs], 
                         mode='markers', name='Detected Anomaly',
                         marker=dict(color='red', size=7)))

"""### Metrics calculation

Finally, we calculate several metrics for the model with one threshold:


*   Confusion matrix
*   Precision
*   Recall
*   F-beta score
"""

from sklearn.metrics import precision_recall_fscore_support

def calculate_metrics(ground_truth: pd.DataFrame, anomalies_idxs: list):
    predictions = pd.DataFrame(index=range(len(ground_truth)), columns=['predicted_anomaly'])
    predictions['predicted_anomaly'] = 0
    predictions.iloc[anomalies_idxs] = 1
    
    confusion_matrix = pd.crosstab(ground_truth.loc[:, 'anomaly'], predictions['predicted_anomaly'], margins=True)
    precision, recall, f1, _ = precision_recall_fscore_support(
        ground_truth.loc[:, 'anomaly'], predictions['predicted_anomaly'], beta=2., average='binary'
    )
    return confusion_matrix, precision, recall, f1

train_conf_matrix, *train_metrics = calculate_metrics(train, train_anomalies_idxs)
train_conf_matrix

print(f'Train:\n Precision: {train_metrics[0]:.3f}\n Recall: {train_metrics[1]:.3f}\n F1 score: {train_metrics[2]:.3f}')

valid_conf_matrix, *valid_metrics = calculate_metrics(valid, valid_anomalies_idxs)
valid_conf_matrix

print(f'Valid:\n Precision: {valid_metrics[0]:.3f}\n Recall: {valid_metrics[1]:.3f}\n F1 score: {valid_metrics[2]:.3f}')

"""## Anomaly detection with dynamic threshold

Dynamic threshold is calculated for each point depending on mean and standart deviation in window around this point

### Threshold calculation

Definition of the window and coefficient for standard deviation, based on which the threshold is calculated
"""

window = 40
std_coef = 6

"""Calculation of the dynamic threshold using the prediction errors for **training** data"""

train_pred_errors_windowed = pd.Series(train_pred_errors).rolling(window=window, min_periods=1)
train_dynamic_threshold = train_pred_errors_windowed.mean() + std_coef * train_pred_errors_windowed.std()

"""Calculation of the dynamic threshold using the prediction errors for **validation** data"""

valid_pred_errors = calculate_prediction_errors(res_valid.view(-1), torch.tensor(valid_values).view(-1), criterion)

valid_pred_errors_windowed = pd.Series(valid_pred_errors).rolling(window=window, min_periods=1)
valid_dynamic_threshold = valid_pred_errors_windowed.mean() + std_coef * valid_pred_errors_windowed.std()

"""### Data filtering

Then, we filter results of the model according to the thresholds and get the **indexes** of detected anomalies
"""

def detect_anomalies(predicted: torch.Tensor, target, upper_bound, criterion):
    anomalies_idxs = []
    for idx, (t, p, u) in enumerate(zip(target, predicted, upper_bound)):
        error = criterion(p.view(-1), t.view(-1)).cpu().numpy().flatten()
        if error > u:
            anomalies_idxs.append(idx)
    return anomalies_idxs

train_anomalies_dynamic_idxs = detect_anomalies(res_train, torch.tensor(train_values), train_dynamic_threshold, criterion)
train_anomalies_dynamic_idxs

valid_anomalies_dynamic_idxs = detect_anomalies(res_valid, torch.tensor(valid_values), valid_dynamic_threshold, criterion)
valid_anomalies_dynamic_idxs

"""Plot of the result for training data:


*   Blue points - non-anomaly data
*   Red points - detected anomaly data
*   Green points - real anomaly data
"""

layout = dict(xaxis=dict(title='Timestamp'), yaxis=dict(title='Battery Temperature'))
fig = go.Figure(layout=layout)
fig.add_trace(go.Scatter(x=train['timestamp'], y=train['value'], 
                         mode='markers', name='Non-anomaly',
                         marker=dict(color='blue')))
fig.add_trace(go.Scatter(x=train_anomalies['timestamp'], y=train_anomalies['value'], 
                         mode='markers', name='Real Anomaly',
                         marker=dict(color='green', size=13)))
fig.add_trace(go.Scatter(x=train['timestamp'][train_anomalies_dynamic_idxs],
                         y=train['value'][train_anomalies_dynamic_idxs], 
                         mode='markers', name='Detected Anomaly',
                         marker=dict(color='red', size=7)))

"""Plot of the result for validation data:


*   Blue points - non-anomaly data
*   Red points - detected anomaly data
*   Green points - real anomaly data
"""

layout = dict(xaxis=dict(title='Timestamp'), yaxis=dict(title='Battery Temperature'))
fig = go.Figure(layout=layout)
fig.add_trace(go.Scatter(x=valid['timestamp'], y=valid['value'], 
                         mode='markers', name='Non-anomaly',
                         marker=dict(color='blue')))
fig.add_trace(go.Scatter(x=valid_anomalies['timestamp'], y=valid_anomalies['value'], 
                         mode='markers', name='Real Anomaly',
                         marker=dict(color='green', size=13)))
fig.add_trace(go.Scatter(x=valid['timestamp'][valid_anomalies_dynamic_idxs],
                         y=valid['value'][valid_anomalies_dynamic_idxs], 
                         mode='markers', name='Detected Anomaly',
                         marker=dict(color='red', size=7)))

"""### Metrics calculation

Finally, we calculate several metrics for the model with dynamic threshold:


*   Confusion matrix
*   Precision
*   Recall
*   F-beta score

Metrics for training data
"""

train_conf_matrix, *train_metrics = calculate_metrics(train, train_anomalies_dynamic_idxs)
train_conf_matrix

print(f'Train:\n Precision: {train_metrics[0]:.3f}\n Recall: {train_metrics[1]:.3f}\n F1 score: {train_metrics[2]:.3f}')

"""Metrics for validation data"""

valid_conf_matrix, *valid_metrics = calculate_metrics(valid, valid_anomalies_dynamic_idxs)
valid_conf_matrix

print(f'Valid:\n Precision: {valid_metrics[0]:.3f}\n Recall: {valid_metrics[1]:.3f}\n F1 score: {valid_metrics[2]:.3f}')
